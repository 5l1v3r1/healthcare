# Lung Cancer Demo

[![Complete this tutorial in Cloud
Shell](http://gstatic.com/cloudssh/images/open-btn.png)](https://console.cloud.google.com/cloudshell/open?git_repo=https://github.com/GoogleCloudPlatform/healthcare.git&page=shell&tutorial=fhir/immunizations_demo/docs/README.md)

## Setup

1. Create a new project on [Google Cloud](https://cloud.google.com).
1. Enable the [Cloud Healthcare API](https://console.cloud.google.com/apis/library/healthcare.googleapis.com).
1. Enable [Cloud AI Platform](https://console.cloud.google.com/apis/library/ml.googleapis.com).
1. Enable [Cloud Functions]()
1. Launch the [Google Cloud Shell](https://console.cloud.google.com).

## Create the model

### Convert the Synthea data to TF records

```bash
PROJECT_ID=`gcloud config get-value project`
DATA_BUCKET=gcp-fhir-demo-dataset
REGION=us-central1
```

Create a new bucket in your project, for simplicity we will use the same name for our bucket as we used for our project.

```bash
gsutil mb -c regional -l ${REGION?} gs://${PROJECT_ID?}
```

Next we will convert the Patient bundles into Tensorflow records. The `assemble_training_data` module inspects each bundle to determine if the Patient was ever diagnosed with lung cancer, and collects weight, smoking status and age as input features.

```bash
python3 -m scripts.assemble_training_data --src_bucket=gs://${DATA_BUCKET?} --src_folder=synthea --dst_bucket=gs://${PROJECT_ID?} --dst_folder=tfrecords
```

Verify that the TF records were generated correctly by listing the contents of the output folder.

```bash
gsutil gs://${PROJECT_ID?}/tfrecords
```

### Train the model

To train the model we pass in the paths to the training and evaluation data we created in the last step, as well as some other training hyperparameters parameters. For a real model this would need to be adjusted. Because this is a linear regression the model will only take a few seconds to train with a CPU.

```bash
python3 -m models.trainer.model \
    --training_data=gs://${PROJECT_ID?}/tfrecords/training.tfrecord \
    --eval_data=gs://${PROJECT_ID?}/tfrecords/eval.tfrecord \
    --model_dir=gs://${PROJECT_ID?}/model \
    --training_steps=3000 \
    --eval_steps=1000 \
    --learning_rate=0.1 \
    --export_model_dir=gs://${PROJECT_ID?}/saved_model
```

### Deploy the model

The saved model has been stored in a new directory in your bucket in the `saved_model` directory, named with a timestamp. Look up the timestamp using `gsutil`, and save it as an environment variable.

```bash
gsutil ls gs://${PROJECT_ID?}/saved_model/
# Output:
#   gs://${PROJECT_ID?}/saved_model/
#   gs://${PROJECT_ID?}/saved_model/nnnnnnnnnn/
TIMESTAMP= # nnnnnnnnnn
```

Now we can create a new model on AI Platform using the snapshot from our trained model.

```bash
export MODEL=devdaysdemo
export VERSION=v1
gcloud ai-platform models create --regions ${REGION?} ${MODEL?}
# Create the model version in the background using --async
gcloud ai-platform versions create ${VERSION?} \
    --async \
    --model ${MODEL?} \
    --origin gs://${PROJECT_ID?}/saved_model/${TIMESTAMP?}
```

The `versions create` operation was launched asynchronously because it will take a few minutes. You can poll the AI Platform operation for the status.

```bash
gcloud ai-platform operations list
```

Once the operation returns `DONE` test our model using some sample data. You can keep going and come back to this step once the model version has been created.

```bash
python3 -m scripts.predict --project ${PROJECT_ID?} --model ${MODEL?} --version ${VERSION?}
```

## Create a FHIR store

Now we need a FHIR store to hold our simulated patients, and to receive and trigger the RiskAssessments.

```bash
DATASET_ID=devdays
FHIR_STORE_ID=lung-cancer
PUBSUB_TOPIC=fhir
```

We are using Cloud PubSub to trigger a Cloud Function that will create the RiskAssessments. The FHIR store will be configured to send a notification to `PUBSUB_TOPIC` for every modification that occurs.

Datasets are used to aggregate data in the Healthcare API by related purpose. We will create a new `devdays` dataset to hold the FHIR store for this demo. We won't be using any other Healthcare data stores in this demo.

```bash
# Create a PubSub topic
gcloud pubsub topics create ${PUBSUB_TOPIC?}
# Create a dataset
gcloud alpha healthcare datasets create ${DATASET_ID?}
# Create the FHIR store
gcloud alpha healthcare fhir-stores create \
  --dataset ${DATASET_ID?} \
  --pubsub-topic "projects/${PROJECT_ID?}/topics/${PUBSUB_TOPIC?}" \
  --enable-update-create \
  ${FHIR_STORE_ID?}
```

REST APIs are also available to create FHIR stores, in addition to the CLI, but `gcloud` is used in this guide because of its brevity.

### Create the Cloud Function

Now we need to create a Cloud Function to transform the predictions that our model is making into a RiskAssessment. We are using Cloud Functions for its simplicity and low cost maintenance, but other options such as App Engine and Cloud Runtime are available.

Run the `deploy.sh` script in `inference/` to deploy the inference code to Cloud Functions. We give it the same name as the model it calls.

```bash
cd inference
# This will take a few minutes
bash deploy.sh --name ${MODEL?} --topic ${PUBSUB_TOPIC?} --env_vars MODEL=${MODEL?},VERSION=${VERSION?}
cd ..
```

The Cloud Function receives the path of a resource that has been modified. If the resource is a resource type that contains information about the features our model has been trained on (Observation, Condition or Patient) the Cloud Function retrieves the Patient bundle for that resource's Patient and extracts the model features. Then it calls the predict on the model we published and gets back a prediction for whether that patient is at risk for developing lung cancer. Finally, the Cloud Function converts the prediction into a RiskAssessment and writes it to the FHIR store.

## Running the pipeline

Set some environment variables that will make the following REST calls easier to read.

```bash
TOKEN="Authorization: Bearer $(gcloud auth print-access-token)"
CT="Content-Type: application/json+fhir; charset=utf-8"
BASE_URL="https://healthcare.googleapis.com/v1beta1/projects/${PROJECT_ID?}/locations/${REGION?}/datasets/${DATASET_ID?}/fhirStores/${FHIR_STORE_ID?}/fhir"
```

Download the data for the Patient we will be analyzing. The demo patient is a 34-year old female, with no history of smoking, who is currently 71 kg.

<!-- Could also be an import -->
```bash
gsutil cp gs://${DATA_BUCKET}/synthea/patient_bundle.json .
curl -X POST -H "$TOKEN" -H "$CT" \
  -d @patient_bundle.json \
  "$BASE_URL"
```

Search for our newly created Patient to ensure she is in the FHIR store

```bash
curl -H "$TOKEN" "$BASE_URL/Patient?name=Amberly"
```

Search for any RiskAssessments. You should see one that was created when we executed the Patient bundle. Amberly's risk will be `negligible`.

```bash
curl -H "$TOKEN" "$BASE_URL/RiskAssessment"
```

Synthea models smoking status as a response to a `Tobacco smoking status NHIS` survey the patient receives during their regular checkups. To change that level of risk,  update one of Amberly's checkups to change her smoking status response from `Never smoker` to `Every day smoker`.

```bash
gsutil cp gs://${DATA_BUCKET}/synthea/smoking_survey.json .
curl -X PUT -H "$TOKEN" -H "$CT" \
  -d @smoking_survey.json \
  "$TOKEN/Observation/a39bb260-4768-4989-8e1b-730c71085f58"
```

Search for the RiskAssessments again. You should see that the original RiskAssessment's risk has been updated from `negligible` to `moderate`.

```bash
curl -H "$TOKEN" "$BASE_URL/RiskAssessment"
```
