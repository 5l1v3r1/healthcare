{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "e4TNC-3LszlF"
      },
      "source": [
        "# Training a Convolutional Neural Network to Classify Chest X-rays"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "JsNl7d9Cy6X4"
      },
      "source": [
        "## Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "T5_Riz79y_ZO"
      },
      "source": [
        "This notebook shows how to train a state of the art Convolutional Neural Network\n",
        "(CNN) to classify chest X-rays images from the MIMIC CXR Dataset. Its approach\n",
        "is influenced by [CheXpert: A Large Chest Radiograph Dataset with Uncertainty\n",
        "Labels and Expert Comparison](https://arxiv.org/abs/1901.07031).\n",
        "\n",
        "You can run this notebook from [Colab](https://colab.research.google.com/) or\n",
        "[Cloud AI Platform Notebook](https://cloud.google.com/ai-platform-notebooks/).\n",
        "If you're serious about training your own models, you'll definitely want to use\n",
        "a Cloud AI Platform notebook with one or more TPUs or GPUs. If you're just\n",
        "interested in learning how to train a CNN, you can run this notebook in Colab.\n",
        "Your Colab session will probably timeout before it can finish training the\n",
        "model (Cloud AI Platform notebooks are more powerful and never timeout). In any\n",
        "case, don't worry, several pretrained models are available along with the data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "-m30o577n9mt"
      },
      "outputs": [],
      "source": [
        "# Use tensorflow 1.14 (should be released very soon)\n",
        "# this notebook is also compatible with tensorflow 2.0\n",
        "!pip install -q tf-nightly-gpu"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "hwfrAocTs92n"
      },
      "outputs": [],
      "source": [
        "from __future__ import division\n",
        "from __future__ import print_function\n",
        "\n",
        "import datetime\n",
        "import os\n",
        "import tensorflow as tf\n",
        "import multiprocessing\n",
        "from enum import Enum\n",
        "from google.cloud import bigquery\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "try:\n",
        "    from google.colab import auth\n",
        "    IN_COLAB = True\n",
        "    auth.authenticate_user()\n",
        "except ImportError:\n",
        "    IN_COLAB = False\n",
        "\n",
        "%config InlineBackend.figure_format = 'svg'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WJ7qFWUisQJD"
      },
      "source": [
        "## Understanding the dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "L2RbPSlZ-bqi"
      },
      "source": [
        "First, we need to specify where the training and validation datasets are\n",
        "located. Labelled images are provided in\n",
        "[TFRecord](https://www.tensorflow.org/guide/datasets#consuming_tfrecord_data)\n",
        "format. TFRecords are a great choice for performant and convenient training. You\n",
        "also have access to BigQuery tables that contain the labels for each image,\n",
        "which we'll use to get a broad understanding of how the labels are distributed\n",
        "before we dive into training our model.\n",
        "\n",
        "There are separate TFRecords for X-rays taken from frontal or lateral views. You\n",
        "can choose either type of dataset, but make sure the validation and training\n",
        "dataset correspond to the same view. There are pretrained models available for\n",
        "both views."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "s6i-MM779FbU"
      },
      "outputs": [],
      "source": [
        "#@title Input Datasets {run: \"auto\"}\n",
        "GCP_ANALYSIS_PROJECT = 'your-analysis-project'  #@param {type: \"string\"}\n",
        "TRAIN_TFRECORDS = 'gs://your-analysis-bucket/mimic-cxr-processed/resized_tfrecord/train/frontal*'  #@param {type: \"string\"}\n",
        "VALID_TFRECORDS = 'gs://your-analysis-bucket/mimic-cxr-processed/resized_tfrecord/valid/frontal*'  #@param {type: \"string\"}\n",
        "# VIEW should be one of 'frontal', or 'lateral'\n",
        "VIEW = 'frontal'  #@param [\"frontal\", \"lateral\"] {type: \"string\"}\n",
        "TRAIN_BIGQUERY = 'your-analysis-project.cxr.processed_labels_train'  #@param {type: \"string\"}\n",
        "VALID_BIGQUERY = 'your-analysis-project.cxr.processed_labels_valid'  #@param {type: \"string\"}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vaN3NKPk-3s8"
      },
      "source": [
        "The dataset consists of labelled images. These labels were determined by\n",
        "analyzing radiologist notes. A label is given the value of\n",
        "\n",
        "*   `0` (`not_mentioned`) if the note made no mention of it\n",
        "*   `1` (`negative`) if the note said the label wasn't present in the image\n",
        "*   `2` (`uncertain`) if the note expressed uncertainty about the label's presence\n",
        "*   `3` (`positive`) if the label was mentioned with certainty in the note\n",
        "\n",
        "for more details about how these labels were generated, you can check out\n",
        "[this paper](https://arxiv.org/abs/1901.07031). For our classifier we'll treat `not_mentioned` (`0`) and `negative` (`1`) as the same thing. There's some choice in how we handle `uncertain` (`2`), we'll look into this in the next section."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "4R8Amh9EyNvG"
      },
      "outputs": [],
      "source": [
        "class Labels(Enum):\n",
        "  enlarged_cardiomediastinum = 0\n",
        "  cardiomegaly = 1\n",
        "  airspace_opacity = 2\n",
        "  lung_lesion = 3\n",
        "  edema = 4\n",
        "  consolidation = 5\n",
        "  pneumonia = 6\n",
        "  atelectasis = 7\n",
        "  pneumothorax = 8\n",
        "  pleural_effusion = 9\n",
        "  pleural_other = 10\n",
        "  fracture = 11\n",
        "  support_devices = 12\n",
        "\n",
        "\n",
        "class LabelValues(Enum):\n",
        "  not_mentioned = 0\n",
        "  negative = 1\n",
        "  uncertain = 2\n",
        "  positive = 3\n",
        "\n",
        "\n",
        "class Views(Enum):\n",
        "  frontal = 0\n",
        "  lateral = 1\n",
        "  other = 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "csxgv426CP6B"
      },
      "source": [
        "Before we start building our model, let's check out the distribution of the\n",
        "data. We'll do this by writing a BigQuery StandardSQL statement that counts the\n",
        "number of `not_mentioned`, `negative`, `uncertain` and `positive` values for\n",
        "each label."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "uYoxsUQvEV6_"
      },
      "outputs": [],
      "source": [
        "bq_client = bigquery.Client(project=GCP_ANALYSIS_PROJECT)\n",
        "\n",
        "queries = []\n",
        "for label in Labels:\n",
        "  queries.append(\"\"\"\n",
        "    SELECT\n",
        "      \"{label}\" AS label,\n",
        "      {label} AS label_value,\n",
        "      COUNT(DISTINCT path) AS cnt,\n",
        "      dataset\n",
        "    FROM\n",
        "      (SELECT *, \"train\" AS dataset FROM `{TRAIN_BIGQUERY}`\n",
        "      UNION ALL\n",
        "      SELECT *, \"valid\" AS dataset FROM `{VALID_BIGQUERY}`)\n",
        "    WHERE view = {view_value}\n",
        "    GROUP BY {label}, dataset\n",
        "    \"\"\".format(\n",
        "        TRAIN_BIGQUERY=TRAIN_BIGQUERY,\n",
        "        VALID_BIGQUERY=VALID_BIGQUERY,\n",
        "        label=label.name,\n",
        "        view_value=Views[VIEW].value))\n",
        "\n",
        "barplot_df = bq_client.query('UNION ALL'.join(queries)).to_dataframe()\n",
        "# Convert integer label values into strings\n",
        "barplot_df.label_value = barplot_df.label_value.apply(lambda v: LabelValues(v).\n",
        "                                                      name)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lqxqYe1Z27wR"
      },
      "source": [
        "Our StandardSQL statement returns a pandas Dataframe in 'long' format, which\n",
        "makes it really easy to visualize with [seaborn](https://seaborn.pydata.org/)\n",
        "(or [ggplot2](https://ggplot2.tidyverse.org/) for R users)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "9qNznWt9Dzbg"
      },
      "outputs": [],
      "source": [
        "sns.catplot(\n",
        "    y='label',\n",
        "    x='cnt',\n",
        "    hue='label_value',\n",
        "    data=barplot_df,\n",
        "    col='dataset',\n",
        "    kind='bar',\n",
        "    sharex=False)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "OmW0g8kI7JHR"
      },
      "outputs": [],
      "source": [
        "N_TRAIN = barplot_df.cnt[(barplot_df.dataset == 'train')\n",
        "                         \u0026 (barplot_df.label == Labels(0).name)].sum()\n",
        "N_VALID = barplot_df.cnt[(barplot_df.dataset == 'valid')\n",
        "                         \u0026 (barplot_df.label == Labels(0).name)].sum()\n",
        "print('training examples: {:,}\\nvalidation examples: {:,}'.format(\n",
        "    N_TRAIN, N_VALID))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "vzyafcOWE7uF"
      },
      "source": [
        "What have we learned from this?\n",
        "\n",
        "*   we have a medium sized dataset for training a CNN. For comparison\n",
        "    [ImageNet](http://www.image-net.org/) has 14 million images, and\n",
        "    [CIFAR-10](https://www.cs.toronto.edu/~kriz/cifar.html) has 60,000 images.\n",
        "*   the distribution of the values varies for each label, the only constant is\n",
        "    that the majority of values are `not_mentioned`.\n",
        "\n",
        "This informs our expectations for any model's performance with each label. It\n",
        "also suggests that we find a way to take advantage of the `uncertain` labels,\n",
        "since in some cases they actually outnumber the `positive` labels."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "F1-b9NXMyykj"
      },
      "source": [
        "## Creating an input pipeline"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "UeKhixlozB9w"
      },
      "source": [
        "### Performance considerations\n",
        "\n",
        "One of the most important factors in determining how long it takes to train a\n",
        "model is the process that loads data into the model. TPUs and GPUs are so fast\n",
        "that keeping them busy with the next batch of data isn't easy.\n",
        "\n",
        "A few tips for fast input pipelines:\n",
        "\n",
        "1.  Use TFRecords: they allow for data to be read in contiguous blocks, which is\n",
        "    much faster than reading a bunch of small files.\n",
        "1.  Train your models and store your data in the cloud so you can take advantage\n",
        "    of Google's fast internal networks.\n",
        "1.  Perform expensive transformations, including resizing large images, ahead of\n",
        "    time. We've already done this for you using\n",
        "    [Cloud Dataflow](https://cloud.google.com/dataflow/).\n",
        "1.  Use the largest batch size that will fit in your device's memory.\n",
        "\n",
        "You can find more tips\n",
        "[here](https://www.tensorflow.org/alpha/guide/data_performance). And if you're\n",
        "using a cloud TPU, you can also use the\n",
        "[`cloud_tpu_profiler`](https://cloud.google.com/tpu/docs/cloud-tpu-tools#profile_tab),\n",
        "which is an incredibly helpful tool for improving your model's performance.\n",
        "\n",
        "### Dealing with uncertain labels (advanced)\n",
        "\n",
        "Our second issue is how to assign values to the uncertain labels. This is a\n",
        "little technical, so feel free to skim through this section.\n",
        "\n",
        "Almost all multi-label neural networks use a loss function like this:\n",
        "\n",
        "$$\n",
        "L = - \\sum_{n,i} l\\left(y_{ni}, t_{ni}\\right)\n",
        "$$\n",
        "\n",
        "Where $t_{ni}$ is the true value of the $i^\\text{th}$ label of the $n^\\text{th}$\n",
        "sample and $y_{ni}$ is the corresponding prediction from our model.\n",
        "\n",
        "One way to incorporate uncertain labels is to assign them a value $u \\in [0, 1]$\n",
        "and a weight $w \\in [0, 1]$ to that our loss function becomes\n",
        "\n",
        "$$\n",
        "L =  - \\sum_{n,i} \\begin{cases}\n",
        "l\\left(y_{ni}, t_{ni}\\right) \u0026 (n, i)^{\\text{th}} \\text{ label is certain} \\\\\n",
        "w \\cdot l\\left(y_{ni}, u\\right) \u0026 (n, i)^{\\text{th}} \\text{ label is uncertain}\n",
        "\\end{cases}\n",
        "$$\n",
        "\n",
        "$w = 0$ corresponds to ignoring the uncertain labels. $w = 1, u = 0$ corresponds\n",
        "to counting all uncertain labels as negative. $w = 1, u = 1$ counts the\n",
        "uncertain labels as positive. Something like $w = 0.5, u = 0.25$ is a hybrid of\n",
        "the others. You can think of $w$ as playing the role of $\\sigma^2$ if $l$ was\n",
        "the log-likelihood of a normal distribution.\n",
        "\n",
        "Other approaches for incorporating uncertain labels into a model are discussed\n",
        "[here](https://arxiv.org/abs/1901.07031). You could also experiment with using\n",
        "different values of $u$ and $w$ for each label. You could even try to optimize these\n",
        "hyperparameters. The per label values of $u$ for example could be learned with\n",
        "gradient descent, while $w$ could be updated every epoch to optimize the loss on\n",
        "the `certain` labels.\n",
        "\n",
        "In summary:\n",
        "\n",
        "*   `U_VALUE` ($u$) is the probability of being `positive` that you assign to\n",
        "    uncertain labels\n",
        "*   `W_VALUE` ($w$) is the weight that you assign to uncertain labels during\n",
        "    training."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "cellView": "both",
        "colab": {},
        "colab_type": "code",
        "id": "udjTuUIy_wUy"
      },
      "outputs": [],
      "source": [
        "#@title Input pipeline parameters {run: \"auto\"}\n",
        "BATCH_SIZE = 32  #@param {type: \"integer\"}\n",
        "NUM_EPOCHS = 3  #@param {type: \"integer\"}\n",
        "U_VALUE = 0.4  #@param {type:\"slider\", min:0, max:1, step:0.01}\n",
        "W_VALUE = 0.75  #@param {type:\"slider\", min:0, max:1, step:0.01}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ctKPP8DXAHfS"
      },
      "outputs": [],
      "source": [
        "# label -\u003e probability table: 0 -\u003e 0, 1 -\u003e 0, 2 -\u003e u, 3 -\u003e 1\n",
        "probabs_lookup = tf.constant([0.0, 0.0, U_VALUE, 1.0])\n",
        "# label -\u003e weight table: 0 -\u003e 1, 1 -\u003e 1, 2 -\u003e w, 3 -\u003e 1\n",
        "weights_lookup = tf.constant([1.0, 1.0, W_VALUE, 1.0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "q_4UP-Wwyx3t"
      },
      "outputs": [],
      "source": [
        "feature_description = {'jpg_bytes': tf.io.FixedLenFeature([], tf.string)}\n",
        "for l in Labels:\n",
        "  feature_description[l.name] = tf.io.FixedLenFeature([], tf.int64)\n",
        "\n",
        "# The height, width, and number of channels of the input images\n",
        "INPUT_HWC = (320, 320, 1)\n",
        "\n",
        "\n",
        "def parse_function(example):\n",
        "  \"\"\"Convert a TFExample from a TFRecord into an input and its true label.\n",
        "\n",
        "    Args:\n",
        "      example (tf.train.Example): A training example read from a TFRecord.\n",
        "\n",
        "    Returns:\n",
        "      Tuple[tf.Tensor, tf.Tensor]: The X-ray image and its labels. The labels\n",
        "        are represented as two stacked arrays. One array is the probability\n",
        "        that this label exists in the image, the other is how much weight this\n",
        "        label should have when training the model.\n",
        "    \"\"\"\n",
        "  parsed = tf.io.parse_single_example(example, feature_description)\n",
        "  # Turn the JPEG data into a matrix of pixel intensities\n",
        "  image = tf.io.decode_jpeg(parsed['jpg_bytes'], channels=1)\n",
        "  # Give the image a definite size, which is needed by TPUs\n",
        "  image = tf.reshape(image, INPUT_HWC)\n",
        "  # Normalize the pixel values to be between 0 and 1\n",
        "  scaled_image = (1.0 / 255.0) * tf.cast(image, tf.float32)\n",
        "  # Combine the labels into an array\n",
        "  labels = tf.stack([parsed[l.name] for l in Labels], axis=0)\n",
        "  # Convert the labels into probabilities and weights using lookup tables.\n",
        "  probs = tf.gather(probabs_lookup, labels)\n",
        "  weights = tf.gather(weights_lookup, labels)\n",
        "  # Return the input to the model and the true labels\n",
        "  return scaled_image, tf.stack([probs, weights], axis=0)\n",
        "\n",
        "\n",
        "def get_dataset(valid=False):\n",
        "  \"\"\"Construct a pipeline for loading the data.\n",
        "\n",
        "    Args:\n",
        "      valid (bool): If this is True, use the validation dataset instead of the\n",
        "        training dataset.\n",
        "\n",
        "    Returns:\n",
        "      tf.data.Dataset: A dataset loading pipeline ready for training.\n",
        "    \"\"\"\n",
        "  n_cpu = multiprocessing.cpu_count()\n",
        "  tf_records = VALID_TFRECORDS if valid else TRAIN_TFRECORDS\n",
        "  dataset = tf.data.TFRecordDataset(\n",
        "      tf.io.gfile.glob(tf_records),\n",
        "      buffer_size=16 * 1024 * 1024,\n",
        "      num_parallel_reads=n_cpu)\n",
        "  if not valid:\n",
        "    dataset = dataset.shuffle(256)\n",
        "  dataset = dataset.repeat()\n",
        "  dataset = dataset.map(parse_function, num_parallel_calls=n_cpu)\n",
        "  dataset = dataset.batch(BATCH_SIZE, drop_remainder=True)\n",
        "  dataset = dataset.prefetch(tf.data.experimental.AUTOTUNE)\n",
        "  return dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-iFsTYJNT7mA"
      },
      "source": [
        "## Using accelerators (TPUs and GPUs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "rG3LEZ_Jmdcj"
      },
      "source": [
        "One of the most exciting things about training neural networks on the cloud is\n",
        "the ability to scale your compute power. Tensorflow makes it easy to scale from\n",
        "a single GPU to a multi-GPU system, to a network of distributed GPU systems.\n",
        "TPUs are even easier. Since all TPUs are distributed systems (with 8 cores and 4\n",
        "chips per board), you can scale from a single TPU to a TPU pod without changing\n",
        "any of your code."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "798l9Ht1UAOd"
      },
      "outputs": [],
      "source": [
        "#@title Accelerators {run: \"auto\"}\n",
        "ACCELERATOR_TYPE = 'Single-GPU'  #@param [\"Single/Multi-TPU\", \"Single-GPU\", \"Multi-GPU\", \"CPU\"] {type: \"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "iKFe1cWM7FB8"
      },
      "outputs": [],
      "source": [
        "if ACCELERATOR_TYPE == 'Single/Multi-TPU':\n",
        "  if IN_COLAB:\n",
        "    tpu_name = 'grpc://' + os.environ['COLAB_TPU_ADDR']\n",
        "  else:\n",
        "    tpu_name = os.environ['TPU_NAME']\n",
        "  resolver = tf.contrib.cluster_resolver.TPUClusterResolver(tpu=tpu_name)\n",
        "  tf.contrib.distribute.initialize_tpu_system(resolver)\n",
        "  strategy = tf.contrib.distribute.TPUStrategy(resolver, steps_per_run=100)\n",
        "elif ACCELERATOR_TYPE == 'Multi-GPU':\n",
        "  strategy = tf.distribute.MirroredStrategy()\n",
        "else:\n",
        "  strategy = tf.distribute.get_strategy()  # Default strategy"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "8udz2vQ_QhWK"
      },
      "source": [
        "## Defining the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "9etia5dIQr2d"
      },
      "source": [
        "The [keras applications module](https://keras.io/applications/) offers several\n",
        "different CNN architectures for us to choose from, all of which are very good.\n",
        "\n",
        "All we have to do is add the last layer which produces a value for each of our\n",
        "labels. Notice that the activation function for this layer is `'linear'`. That's\n",
        "because our loss function applies its own `sigmoid` nonlinearity to `y_pred`.\n",
        "\n",
        "We define a custom loss function, that unpacks the true probabilities and\n",
        "assigned weights from our input pipeline before using these to compute a\n",
        "weighted cross-entropy loss."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "i0YyPiGB9vxj"
      },
      "outputs": [],
      "source": [
        "with strategy.scope():\n",
        "  base_model = tf.keras.applications.densenet.DenseNet121(\n",
        "      include_top=False, weights=None, input_shape=INPUT_HWC, pooling='max')\n",
        "\n",
        "  predictions = tf.keras.layers.Dense(\n",
        "      len(Labels), activation='linear')(\n",
        "          base_model.output)\n",
        "\n",
        "  model = tf.keras.Model(inputs=base_model.input, outputs=predictions)\n",
        "\n",
        "  def weighted_binary_crossentropy(prob_weight_y_true, y_pred):\n",
        "    \"\"\"Binary cross-entropy loss function with per-sample weights.\"\"\"\n",
        "    prob_weight_y_true = tf.reshape(prob_weight_y_true, (-1, 2, len(Labels)))\n",
        "    # Unpack the second output of our data pipeline into true probabilities and\n",
        "    # weights for each label.\n",
        "    probs = prob_weight_y_true[:, 0]\n",
        "    weights = prob_weight_y_true[:, 1]\n",
        "    return tf.compat.v1.losses.sigmoid_cross_entropy(\n",
        "        probs,\n",
        "        y_pred,\n",
        "        weights,\n",
        "        reduction=tf.compat.v1.losses.Reduction.SUM_OVER_BATCH_SIZE)\n",
        "\n",
        "  model.compile(\n",
        "      optimizer=tf.train.AdamOptimizer(),\n",
        "      loss=weighted_binary_crossentropy,\n",
        "  )"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "4zuXzbe4tGGv"
      },
      "source": [
        "## Training the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QBagIQwEAs8z"
      },
      "source": [
        "You can track the training by saving the training log files and running\n",
        "tensorboard in a Google Cloud Shell or on your local machine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "cellView": "both",
        "colab": {},
        "colab_type": "code",
        "id": "eCu8LYTxA0tJ"
      },
      "outputs": [],
      "source": [
        "#@title GCS Tensorboard log directory {run: \"auto\"}\n",
        "GCS_LOGS = 'gs://your-analysis-bucket/tensorboard/mimic_cxr/'  #@param {\"type\": \"string\"}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "ArdQITx5BAVm"
      },
      "outputs": [],
      "source": [
        "now_str = datetime.datetime.now().strftime('%Y%m%d-%H%M%S')\n",
        "callbacks = []\n",
        "if GCS_LOGS:\n",
        "  LOGDIR = GCS_LOGS + ('' if GCS_LOGS.endswith('/') else '/') + now_str\n",
        "  callbacks.append(tf.keras.callbacks.TensorBoard(LOGDIR, update_freq=500))\n",
        "  print('Run `tensorboard --logdir {}` in cloud shell or your local machine.'\n",
        "        .format(GCS_LOGS))\n",
        "\n",
        "model.fit(\n",
        "    get_dataset(),\n",
        "    epochs=NUM_EPOCHS,\n",
        "    steps_per_epoch=N_TRAIN // BATCH_SIZE,\n",
        "    validation_data=get_dataset(valid=True),\n",
        "    validation_steps=N_VALID // BATCH_SIZE,\n",
        "    callbacks=callbacks)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "O3TQzOPGtQwV"
      },
      "source": [
        "## Saving the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "OE6nSWMsC6fs"
      },
      "source": [
        "All that's left is to save a copy of our model on GCS."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "YuAurxVe_nqW"
      },
      "outputs": [],
      "source": [
        "#@title Saved model files {run: \"auto\"}\n",
        "MODEL_NAME = 'my_model.h5'  #@param {type: \"string\"}\n",
        "GCS_SAVED_MODEL_DIR = 'gs://your-analysis-bucket/mimic-cxr-models'\n",
        "GCS_MODEL_PATH = GCS_SAVED_MODEL_DIR\n",
        "if not GCS_MODEL_PATH.endswith('/'):\n",
        "  GCS_MODEL_PATH += '/'\n",
        "GCS_MODEL_PATH += MODEL_NAME"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "teO8ts3jAohN"
      },
      "outputs": [],
      "source": [
        "model.save(MODEL_NAME)\n",
        "with tf.io.gfile.GFile(GCS_MODEL_PATH, 'wb') as gcs_model_file:\n",
        "  with open(MODEL_NAME, 'rb') as local_model_file:\n",
        "    gcs_model_file.write(local_model_file.read())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EsCwiZLTzDfN"
      },
      "source": [
        "## Making and evaluating predictions\n",
        "\n",
        "TODO(reidhayes) add code for loading a pretrained models from GCS, running predictions of the evalutation dataset, and plotting PR and ROC curves."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "15WI0XNhDRc7"
      },
      "source": [
        "## Conclusion\n",
        "\n",
        "That's it! You've trained a Convolutional Neural Network to classify chest X-ray\n",
        "images.\n",
        "\n",
        "We've just scratched the surface of machine learning on GCP. The [Cloud AI Platform]( https://cloud.google.com/ai-platform/) has a full suite of tools for research to production ML development including preprocessing pipelines, jupyter notebooks, distributed training, and model serving."
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "last_runtime": {
        "build_target": "//learning/brain/python/client:tpu_hw_notebook",
        "kind": "private"
      },
      "name": "mimic_cxr_train.ipynb",
      "provenance": [
        {
          "file_id": "1gN9P9owTsXPMLlcP3lonYLGI4ERQqlk1",
          "timestamp": 1557769834852
        }
      ],
      "version": "0.3.2"
    },
    "kernelspec": {
      "display_name": "Python 2",
      "name": "python2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
