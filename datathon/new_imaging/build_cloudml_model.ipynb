{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "lUWAmd3akkJo"
      },
      "source": [
        "This notebook will walk you through creating a tensorflow model for batch predictions with [Cloud ML Engine](https://cloud.google.com/ml-engine/docs/).\n",
        "\n",
        "If you're new to Jupyter notebooks/Colab:  you can run  a block (a.k.a cell) of code by selecting it with your mouse and press SHIFT and ENTER at the same time to run it.\n",
        "\n",
        "For more details, including how to view this file and connect it to a python interpreter check out [Welcome to Colab](https://colab.research.google.com/notebooks/welcome.ipynb)."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "jqCeIwb69RP-"
      },
      "source": [
        "# Getting batch predictions from a pretrained model with Cloud ML Engine"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "TmYlRfuH95tE"
      },
      "source": [
        "## Introduction\n",
        "Medical images are scarce. For some rare diseases there may only be tens of cases that are available for imaging. Even when there are many patients, imaging can still be a burden to patients and a cost to healthcare systems.\n",
        "\n",
        "To understand an image, our model will need general perception skills. It will need representations for lines and shapes at different levels of complexity. Because our medical data is so precious, it is best not to waste it on teaching our model such basic skills. Also, our model wouldn't get very good at perception if it only learned from a small set of images.\n",
        "\n",
        "In this notebook, we're going to walk through reducing images into a language of lines and shapes using [Cloud ML Engine](https://cloud.google.com/ml-engine/docs/) and a model that has already learned the skills of perception from [the sea of images on the web](http://www.image-net.org/)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "QTLBHDMl9SAp"
      },
      "outputs": [],
      "source": [
        "import tensorflow as tf\n",
        "tf.keras.backend.set_learning_phase(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "fK0gJTyi9mrx"
      },
      "source": [
        "## The plan\n",
        "\n",
        "Our goal is to convert each image in the dataset into a compact vector representation.\n",
        "The image we start with has complicated correlations between its pixels that form blobs, lines and shadows.\n",
        "This makes for some redundancy in the information each pixel tells us.\n",
        "\n",
        "Neural networks attempt to sequentially remove this redundancy until only the information they care about is left. Often this information is a label, like `apple` or `cat`. Convolutional neural networks (CNN) are a particular species of neural networks that are designed to take advantage of the fact that moving an object around in an image doesn't change its identity.\n",
        "\n",
        "We're going to use a CNN, called [Xception](https://arxiv.org/pdf/1610.02357.pdf), that has already been trained to recoginize things like apples and cats using data from [ImageNet](http://www.image-net.org/). As Xception sequentially reduces our image towards an extremely compact represention (such as the label `apple`), we're going to pull out somewhere in the middle. This will give us a summary of our image as a 2048 dimensional vector. Furthermore we expect the correlations between each of the elements (i.e. features) of this vector to be small.\n",
        "\n",
        "In summary, we're going to accomplish two things\n",
        "1. Supplement our dataset with all the images in ImageNet\n",
        "2. Reduce the dimensionality of our data to avoid [overfitting](https://en.wikipedia.org/wiki/Overfitting) without ignoring the special stucture our data has since it's an image."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "bg6CfMw79YJh"
      },
      "source": [
        "## Implementing the plan\n",
        "\n",
        "Running data through state of the art neural networks can take a lot of computing horse power. Although we won't get into it in this notebook, training (i.e. optimizing the paramaters of) the network is even more demanding.\n",
        "\n",
        "Fortunately, [Cloud ML Engine](https://cloud.google.com/ml-engine/docs/) can take care of balancing the work across an effectively unlimited number of GPUs or TPUs so we can get our results quickly.\n",
        "\n",
        "In particular, we're going to use Cloud ML's [batch prediction feature](https://cloud.google.com/ml-engine/docs/tensorflow/batch-predict) to quickly get feature vectors for every image in our dataset."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Req8EsXf_YGY"
      },
      "source": [
        "### Loading the images\n",
        "\n",
        "The first step in defining the model is to define how to load data. Cloud ML engine supports JSON and [TFRecord](https://www.tensorflow.org/guide/datasets#consuming_tfrecord_data) file formats for input into batch prediction.\n",
        "\n",
        "We define functions to build inputs using both of these datatypes."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "hjrbiznC-A2_"
      },
      "outputs": [],
      "source": [
        "def example_serving_input_fn():\n",
        "  \"\"\"Creates tensors for TFExample input from TFRecord files.\n",
        "\n",
        "  Returns:\n",
        "    Tuple[Dict[str, tf.Tensor], Dict[str, tf.Tensor]]:\n",
        "      The tensorflow tensors for the first and second stage of data\n",
        "      ingestion respectively. In the first stage, data is loaded using\n",
        "      standard Cloud ML input conventions for the TFRecord data type.\n",
        "      In the second stage, the data from the first stage is converted\n",
        "      in the form we have defined as input for our model.\n",
        "  \"\"\"\n",
        "  feature_spec = {key: tf.FixedLenFeature(shape=[], dtype=tf.string)\n",
        "                  for key in ('png_bytes', 'study', 'series', 'instance')}\n",
        "\n",
        "  example_bytestring = tf.placeholder(tf.string, shape=[None])\n",
        "  input_tensors = tf.parse_example(example_bytestring, feature_spec)\n",
        "  return {'example': example_bytestring}, input_tensors\n",
        "\n",
        "# Not used, but included to demonstrate how JSON input works\n",
        "def json_serving_input_fn():\n",
        "  \"\"\"Creates tensors for JSON input.\n",
        "  \"\"\"\n",
        "  # png_bytes is automatically decoded from base64 by Cloud ML Engine.\n",
        "  # Its naming is important. See \"Binary data in prediction input\" in\n",
        "  # https://cloud.google.com/ml-engine/docs/tensorflow/online-predict#formatting_instances_as_json_strings\n",
        "  # for details.\n",
        "  input_tensors = {key: tf.placeholder(tf.string, shape=[None], name=key)\n",
        "                   for key in ('png_bytes', 'study', 'series', 'instance')}\n",
        "  return input_tensors, input_tensors\n",
        "\n",
        "serving_input_fn = example_serving_input_fn\n",
        "\n",
        "simple_input_signature, input_tensors = serving_input_fn()\n",
        "output_tensors = dict()\n",
        "# Instance keys, see\n",
        "# https://cloud.google.com/ml-engine/docs/tensorflow/prediction-overview#instance_keys\n",
        "# for more information\n",
        "for instance_key in ('study', 'series', 'instance'):\n",
        "  output_tensors[instance_key] = tf.identity(input_tensors[instance_key])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "QOVJQexu-ZYX"
      },
      "source": [
        "### Formatting the images\n",
        "\n",
        "The next step is converting binary png encoded image data into an array of pixel intensities.\n",
        "\n",
        "Different neural networks require their data to be in different formats. In particular, the [version of Xception](https://keras.io/applications/#xception) that we're going to use requires its data to be a $299 \\times 299 \\times 3$ array of pixels with values between $-1$ and $+1$. The first two dimensions are the height and width of the image (i.e. its _resolution_), the last dimension is the three primary colors (red, green, blue) that compose each pixel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "9CxnBvAa-G4N"
      },
      "outputs": [],
      "source": [
        "def parse_png(png_bytes, height=299, width=299):\n",
        "  \"\"\"Decode a blob of png bytes and resize to fit CNN input size.\n",
        "\n",
        "  Returns:\n",
        "    tf.Tensor:\n",
        "      A (H=height)(W=width)(C=3) array with pixel\n",
        "      intensities in the range [-1, 1].\n",
        "  \"\"\"\n",
        "  # Decode the png_bytes into a HW(C=3) array.\n",
        "  u8image = tf.image.decode_png(png_bytes, channels=3)\n",
        "  # Resized image pixel instansities are still in [0, 255], but are\n",
        "  # promoted to float32's by the bilinear averaging.\n",
        "  f32image_resized = tf.image.resize_images(u8image, (height, width))\n",
        "  # Convert to data with pixel intensities in the range [-1, 1].\n",
        "  tf_image = tf.keras.applications.xception.preprocess_input(f32image_resized)\n",
        "  return tf_image"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "4UHGUJ3u-gJy"
      },
      "outputs": [],
      "source": [
        "processed_images = tf.map_fn(parse_png,\n",
        "                             input_tensors['png_bytes'],\n",
        "                             dtype=tf.float32,\n",
        "                             back_prop=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "woXgDlWs_Cb-"
      },
      "source": [
        "### Running the neural network\n",
        "\n",
        "The last step in defining the model is defining how the CNN will run on the data. Here's a high level description of each of the arguments\n",
        "\n",
        "* `include_top=False`:  _stop early! I don't want a label like apple, I want a vector summary of the image_\n",
        "* `pooling=max`:  _do one more summarizing step so I get a 2048 dimensional vector_\n",
        "* `weights=imagenet`:  _use a version of this model that has already been trained using images from ImageNet_\n",
        "* `input_tensor=processed_images`:  _this is the tensor holding the input for the CNN_"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "3G2EuXjQ_FqY"
      },
      "outputs": [],
      "source": [
        "fv_model = tf.keras.applications.xception.Xception(\n",
        "    include_top=False,\n",
        "    pooling='max',\n",
        "    weights='imagenet',\n",
        "    input_tensor=processed_images)\n",
        "\n",
        "output_tensors['feature_vector'] = fv_model.output"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "E11kACWL_R5J"
      },
      "source": [
        "Congratulations! Our model is built!\n",
        "Let's save it to a directory so that we can upload it to Cloud ML engine."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "YsK3OMDtD0Z5"
      },
      "outputs": [],
      "source": [
        "tf.saved_model.simple_save(\n",
        "     # the tensorflow session to save\n",
        "    tf.keras.backend.get_session(),\n",
        "    # path to the model, the 1 stands for the model version number by convention\n",
        "    'xception_fv/1',\n",
        "    inputs=simple_input_signature,\n",
        "    outputs=output_tensors)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "zbV9iAfqBBW7"
      },
      "source": [
        "### Getting batch predictions with Cloud ML Engine\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "talaq2AlBjhr"
      },
      "source": [
        "For the curious, we can inspect our model using the [saved_model_cli](https://www.tensorflow.org/guide/saved_model#cli_to_inspect_and_execute_savedmodel) tool that packaged with tensorflow.\n",
        "\n",
        "You can also test out your model at the command line with this tool, which is useful for debugging."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "dq3DTG1jBkV2"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "saved_model_cli show --dir xception_fv/1 --all"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "pNLMqWKXBnYG"
      },
      "source": [
        "Upload the model to Google Cloud Storage (GCS) so that Cloud ML engine will be able to see it. Remeber to edit the below `gcs_model_path` paramater so that it points to one of your GCS buckets. You can do this by editing the code directely, or, if you're using this notebook from Colab, editting the parameter box on the right. Remeber to press SHIFT+ENTER to run it."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "jGd9ynIMLvDh"
      },
      "outputs": [],
      "source": [
        "# Replace me!\n",
        "gcs_model_path = 'gs://my_bucket/xception_fv/1' #@param"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "v_kRnM3GBr0S"
      },
      "outputs": [],
      "source": [
        "%%bash -s {gcs_model_path}\n",
        "gsutil rsync -r xception_fv/1 $1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "S34_bS1VBwUe"
      },
      "source": [
        "Next, we'll load the model into Cloud ML engine.\n",
        "You can also do this using the [Cloud ML engine web interface](https://console.cloud.google.com/mlengine (select `Models` and then `+New Model`)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "TLv7BsMLBx5e"
      },
      "outputs": [],
      "source": [
        "%%bash -s {gcs_model_path}\n",
        "gcloud ml-engine models create xception_fv \\\n",
        "    --description=\\\n",
        "\"Extract feature vectors from png encoded images served in a TFRecord \"\\\n",
        "\"using the Xception CNN trained on imagenet\"\n",
        "\n",
        "gcloud ml-engine versions create version_1 \\\n",
        "    --model=xception_fv \\\n",
        "    --origin=$1"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "cjV06PKeB1Yd"
      },
      "source": [
        "What remains is to launch a batch prediction job to run our model all all the images in your dataset.\n",
        "\n",
        "To create the input, you can use the provided Cloud Dataflow pipeline `png_to_tfrecord.py`.\n",
        "TFExamples of the required for form are be built using code like this:\n",
        "\n",
        "```\n",
        "def _bytes_feature(value):\n",
        "    return tf.train.Feature(bytes_list=tf.train.BytesList(value=[value]))\n",
        "\n",
        "example = tf.train.Example(\n",
        "    features=tf.train.Features(feature={'study': _bytes_feature(study_uid),\n",
        "                                        'series': _bytes_feature(series_uid),\n",
        "                                        'instance': _bytes_feature(instance_uid),\n",
        "                                        'png_bytes': _bytes_feature(png_bytes)}))\n",
        "```\n",
        "\n",
        "And these examples are then saved to a TFRecord file.\n",
        "\n",
        "\n",
        "##### Aside: Using JSON input instead of TFRecord\n",
        "\n",
        "If you want to rebuild this model so that it uses JSON input instead of TFRecord input\n",
        "change `serving_input_fn = example_serving_input_fn` to `serving_input_fn = json_serving_input_fn` in this notebook, rebuild the model and supply your input in a text file with lines like this (use `--data-format=text`\n",
        "for this)\n",
        "\n",
        "```\n",
        "{\"study\": study_uid, \"series\": series_uid, \"instance\": instance_uid, \"png_bytes\": {\"b64\": base64.b64encode(png_bytes)}}\n",
        "```\n",
        "\n",
        "Cloud ML Engine will automatically decode the base64 binary string, see the heading \"Binary data in prediction input\" in [this document](https://cloud.google.com/ml-engine/docs/tensorflow/online-predict#formatting_instances_as_json_strings) for details.\n",
        "##### End aside"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "rBuni9BdNXpz"
      },
      "outputs": [],
      "source": [
        "job_name = 'feature_vector_batch_predict' #@param\n",
        "# Replace me!\n",
        "region = 'us-central1' #@param\n",
        "input_path = 'gs://my_bucket/input_tfrecords/**.tfrecord' #@param\n",
        "output_path = 'gs://my_bucket/cloudml_output/feature_vectors.txt' #@param"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 0,
      "metadata": {
        "colab": {},
        "colab_type": "code",
        "id": "dsh-EApjCSoP"
      },
      "outputs": [],
      "source": [
        "%%bash -s {job_name} {input_path} {output_path} {region}\n",
        "gcloud ml-engine jobs submit prediction $1 \\\n",
        "  --model=xception_fv \\\n",
        "  --input-paths=$2 \\\n",
        "  --output-path=$3  \\\n",
        "  --region=$4 \\\n",
        "  --data-format=tf-record"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r-mfW4MpCpCD"
      },
      "source": [
        "You can watch the progress of the job on the [Cloud ML dashboard](https://console.cloud.google.com/mlengine). When its done, you will find the output on Cloud Storage in the `${output_path}` you provided The format of the output will be a text file with a JSON entry on each line of the form\n",
        "\n",
        "```\n",
        "{\"study\": study_uid, \"series\": series_uid, \"instance\": instance_uid, \"feature_vector\": [1.61,-0.35,..]}\n",
        "```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-USlJngMCunl"
      },
      "source": [
        "## What now?\n",
        "\n",
        "Now that we have a feature vectors describing each of the images in our dataset, the challenges of working with images (high-dimensionality and complicated correlations) are mostly abated. We are free to use the feature vector as a representation of the image in any statistical analysis of our data.\n",
        "\n",
        "Some options:\n",
        "\n",
        "* Classification: Logistic regression, support vector machines, dense neural networks\n",
        "* Visualization: t-SNE, PCA\n",
        "* Clustering: k-means, PCA + guassian mixtures\n",
        "\n",
        "No matter what you choose for the next step of your analysis, loading your data into BigQuery is a great next step. From BigQuery you can join your image feature vectors with DICOM metadata and electronic healthrecords. You can export your queries to a huge variety of formats, or train your models directly using [BigQuery ML](https://cloud.google.com/bigquery/docs/bigqueryml-intro). To this end, we've provided [load_feature_vectors.py](load_feature_vectors.py): a Cloud Dataflow pipeline for loading the text file of feature vectors given to you by Cloud ML engine into BigQuery."
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "build_cloudml_featurevector_model.ipynb",
      "provenance": [
        {
          "file_id": "/piper/depot/google3/third_party/cloud/healthcare/datathon/new_imaging/build_cloudml_model.ipynb?workspaceId=reidhayes:imaging-datathon-int::citc",
          "timestamp": 1544816524450
        },
        {
          "file_id": "/piper/depot/google3/third_party/cloud/healthcare/datathon/new_imaging/Untitled2.ipynb?workspaceId=reidhayes:imaging-datathon-int::citc",
          "timestamp": 1544814029636
        },
        {
          "file_id": "17YvVd8AH6jQ170gwkLJkjoWRvojRiStv",
          "timestamp": 1544811565257
        }
      ],
      "version": "0.3.2"
    },
    "kernelspec": {
      "display_name": "Python 2",
      "name": "python2"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
